\documentclass[11pt]{article}

\usepackage{fullpage} % Package to use full page
\usepackage{parskip} % Package to tweak paragraph skipping
\usepackage{tikz} % Package for drawing
\usepackage{amsmath}
\usepackage[breaklinks]{hyperref}
\usepackage{url}
\usepackage{breakurl} 
\usepackage{enumerate}
\usepackage{subcaption}
\usepackage{mathptmx}
\usepackage{epsfig}
\usepackage{multirow}
%\usepackage{minipage}
\def \hfillx {\hspace*{-\textwidth} \hfill}

\title{Hand-written Digits Classification and Letter Recognition}
\author{Group 10 \\ Sixiang Ma, Yuan Xiao, Xiaokuan Zhang}
\date{12/09/2016}



\begin{document}

\maketitle
\section*{Abstract}
say something

\section{Introduction}

\subsection{Problem Description}
Our group wants to solve the problem of classifying hand-written digits and  black-and-white rectangular pixel-displayed letters. We have two datasets from UCI machine learning repository \cite{Lichman2013}. One is hand-written digits dataset \cite{digitdataset}. Another is letter recognition dataset \cite{letterdataset}. We will introduce them in more detail in Section \ref{exp}.

\subsection{Classifying Algorithms}
In this report, we test three machine learning  algorithms, i.e., Support Vector Machine (SVM), Neural Networks, and Naive Bayes. We will present the detail of algorithms in Section \ref{sec:metho}.
\subsection{Result Summary}
For the first dataset, after tuning the parameters, the three classifiers can achieve , , , accuracy. 

\section{Background}
The hand-written digit dataset was first used in \cite{kaynak1995methods}. In the paper they combined different classifiers to obtain a better performance.  The letter recognition dataset was first used in \cite{frey1991letter}. They generated classification rules to distinguish different letters.

\begin{figure}[htbp]
\centering

\begin{subfigure}[htbp]{0.32\columnwidth}
\includegraphics*[width=\textwidth]{fig/ex_digit}
\caption{A Digit Example: 2}
\label{fig:ex:digit}
\end{subfigure}
\hfill
\begin{subfigure}[htbp]{0.32\columnwidth}
\includegraphics*[width=\textwidth]{fig/ex_letter}
\caption{Letter Examples}
\label{fig:ex:letter}
\end{subfigure}
\hfill
\begin{subfigure}[htbp]{0.32\columnwidth}
\includegraphics*[width=\textwidth]{fig/feature_letter}
\caption{Features of Letters}
\label{fig:feature:letter}
\end{subfigure}
\caption{Examples and Features}
\end{figure}

\section{Methodology}\label{sec:metho}

\subsection{Support Vector Machine}
Support vector machines (SVMs) \cite{cortes1995support} are supervised learning models that analyze data for classicifation or regression. Provided with a set of training examples, which are marked with their belonging categories, a SVM algorithm performs to build a model so as to recognize and assign testing examples to the predicted categories. 

Besides linear classification, SVMs are also able to efficiently perform non-linear classification when kernel trick is involved, whcih will map original input into high-dimensional attriubte space.

\subsection{Neural Networks}
Neural networks, or artificial neural networks \cite{hagan1996neural}, simulate the functions of nerve cells of human brain and serve as an important computational approach in machine learning. They typically form a structure of multiple layers of basic perceptrons and support both supervised and unsupervised learning.

Neural networks have a long history, dating back to the 1940s \cite{mcculloch1943logical}. However, the idea of artificial neural networks was not popular at early days due to its limitation in solving logical calculations \cite{minsky1988perceptrons}. Modern neural networks revived in the past decade, along with the rise of deep learning \cite{bengio2009learning, schmidhuber2015deep}.

\subsection{Naive Bayes}
Naive Bayes classifier makes use of the Bayes Theorem. It is basically a conditional probability model. It is one of the simplest machine learning algorithms. Compared to Bayesian Networks, Naive Bayes is technically a special case by assuming that all features are conditionally independent from each other given the class label. One of the earliest papers that described this algorithm was from 1970s \cite{duda1973pattern}. 

\section{Experiment}\label{exp}
In this project, we mainly used \texttt{Weka} \cite{hall2009weka} to test different machine learning algorithms. For each algorithm, we use different parameter settings, which will be covered in this section.

\subsection{Datasets}
We have two datasets, the hand-written digits dataset \cite{digitdataset} and the letter recognition dataset \cite{letterdataset}. The hand-written digits dataset has 5620 instances. For each instance, there are 1024 attributes (32x32 matrix), whose values are either 0 or 1. An example is shown in Figure \ref{fig:ex:digit}, which is clearly a `2'. After grouping every 4x4 blocks, the dimension is reduced to 64 (8x8 matrix), and each attribute ranges from 0 to 16. Another is letter recognition dataset \cite{letterdataset}. The character images were based on 20 different fonts. It has 20000 instances. For each instance, there are 16 attributes, whose values range from 0 to 15. The attributes are sophisticated, as shown in Figure \ref{fig:feature:letter}. Some examples of the fonts are shown in Figure \ref{fig:ex:letter}. More details of the datasets and features can be found by visiting the links.


\begin{figure}[htbp]
\centering
\begin{subfigure}[htbp]{0.46\columnwidth}
\includegraphics*[width=\textwidth]{fig/digit_svm}
\caption{Digit Classification}
\label{fig:digit-svm}
\end{subfigure}
\hfill
\begin{subfigure}[htbp]{0.46\columnwidth}
\includegraphics*[width=\textwidth]{fig/letter_svm}
\caption{Letter Recognition}
\label{fig:letter-svm}
\end{subfigure}
\hfill
\begin{subfigure}[htbp]{0.46\columnwidth}
\includegraphics*[width=\textwidth]{fig/smo_libsvm}
\caption{Letter Recognition: SMO vs LibSVM}
\label{fig:smo-libsvm}
\end{subfigure}
\caption{Performance of Support Vector Machine}
\label{fig:svm}
\end{figure}

\subsection{Hand-written Digits Classification}
\subsubsection{Support Vector Machine}

As shwon in Figure \ref{fig:digit-svm}, the SVM classifier has a good performance on the Digit dataset. When the polynomial kernel is applied, 98.968\% accuracy can be achieved , and 97.9181\% accuracy is achieved in cases of the linear kernel. In our experiment, two key parameters are tuned. One is the kernel type and the other is the complexity parameter. For kernels, the linear kernel and the polynomial kernel are tested. 

From the figure, two results can be observed. Firstly, polynomial kernel has better performance than the linear one, regardless of values of the complexity parameter. Secondly, the complexity parameter has little effect on the SVM model in regard with the Digit dataset. 

\subsubsection{Neural Networks}
In order to understand how the many parameters influence the performance of multilayer perceptron in classification, we decide to change only one parameter at a time. The default setting is ``-L 0.3 -M 0.2 -N 500 -V 0 -S 0 -E 20 -H a". And the parameters that we are interested in are learning rate, training time and validation threshold.

Figure \ref{fig:digit-ann} shows the average performance for different parameter combinations. For default settings, we have the average accuracy of 92.73\%. And the root relative squared error is as low as 38.53\%. Changing validation threshold to either more or less does not affect the performance at all. And if we let training time be less, the overall accuracy lowers a little, which matches our expectation.

It can be easily noticed that changing learning rate to 0.6 affects the performance heavily. Thus we tested with the same parameter combination again. In Figure \ref{fig:digit-ann-special}, when we ran the test again, the accuracy grew to 98.36\%. Such inconsistency in the test result showed that the performance of multipayer perceptron is influenced by stochastic. If we increase the stochastic by modifying the parameters, the fluctuation grows.

\begin{figure}[htbp]
\centering

\begin{subfigure}[htbp]{0.46\columnwidth}
\includegraphics*[width=\textwidth]{fig/digit_ANN}
\caption{Digit Classification}
\label{fig:digit-ann}
\end{subfigure}
\hfill
\begin{subfigure}[htbp]{0.46\columnwidth}
\includegraphics*[width=\textwidth]{fig/digit_ANN_special}
\caption{L=0.6 in Digit Classification}
\label{fig:digit-ann-special}
\end{subfigure}
\hfill
\begin{subfigure}[htbp]{0.46\columnwidth}
\includegraphics*[width=\textwidth]{fig/letter_ANN}
\caption{Letter Recognition}
\label{fig:letter-ann}
\end{subfigure}
\caption{Performance of Multilayer Perceptron}
\label{fig:ann}
\end{figure}

\subsubsection{Naive Bayes}
The Naive Bayes works well on the Hand-written Digit dataset. In Figure \ref{fig:bayes}, K means whether to use kernel estimator, and D means whether to use supervised discretization. In Figure \ref{fig:digit-bayes}, when K=F, D=F, which is the worst case scenario, the accuracy and F-measure are still higher than 90\%. In the best case (K=T, D=F), they both exceeds 92\%.

\begin{table}[!htb]
%\begin{subtable}

\centering
\begin{minipage}{0.46\columnwidth}
\begin{tabular}{c  c  c} \hline
% centering table
% creating 10 columns
% inserting double-line 

Parameters & Class (Letter) & Accuracy \\\hline
\multirow{3}{*}{K = linear, C = 1} & S & 68\% \\
	& H & 64.4\% \\
	& S & 65\% \\\hline
\multirow{3}{*}{K = poly, C = 1} & H & 91\% \\
	& H & 69.1\% \\
	& S & 64\% \\\hline
\multirow{3}{*}{K = linear, C = 2} & S & 67.5\% \\
	& H & 65.1\% \\
	& S & 64.4\% \\\hline
\multirow{3}{*}{K = poly, C = 2} & H & 91.1\% \\
	& H & 65.1\% \\
	& S & 64.4\% \\\hline
\end{tabular}
\caption{Worst Cases: Support Vector Machine} % title name of the table
\label{tbl:svm}
\end{minipage}
\hfill

\end{table}


\subsection{Letter Recognition}
\subsubsection{Support Vector Machine}

Compared with the Digit dataset, the polynomial kernel still has a good performance on the Letter dataset as more than 95\% instances are correctly recogonized. However, the accurarys from the linear kernel are much lower no matter which values of the complexity parameter are configured. From Figure \ref{fig:letter-svm}, we can see that only 85\% accuracy and F-measure are achieved when the linear kernel is used. 


\subsubsection{Neural Networks}
The overall performance of multilayer perceptron is quite stable. The accuracy is around 82\% and the root relative squared error is around 56\%. However, there are some classes with relatively low accuracy making the average performance not so good as digit recognition. Figure \ref{tbl:ann} shows the least correct classes: `G', `H' and `S'.

Neural networks do a better job than the other two algorithms in recognizing `O' and `Q' and it can correctly tell `X' apart from `S'. Changing learning rate higher helps the performance with `H'.


\begin{table}[!htb]
%\begin{subtable}

\centering
\begin{minipage}{0.46\columnwidth}
\begin{tabular}{c  c  c} \hline
% centering table
% creating 10 columns
% inserting double-line 

Parameters & Class (Letters) & Accuracy \\\hline
\multirow{3}{*}{Default} & G & 69\% \\
	& H & 64.4\% \\
	& S & 65\% \\\hline
\multirow{3}{*}{L = 0.6} & G & 68.2\% \\
	& H & 69.1\% \\
	& S & 64\% \\\hline
\multirow{3}{*}{N = 200} & G & 69.3\% \\
	& H & 65.1\% \\
	& S & 64.4\% \\\hline
\end{tabular}
\caption{Worst Cases: Multilayer Perceptron} % title name of the table
\label{tbl:ann}
\end{minipage}
\hfill
\begin{minipage}{0.46\columnwidth}
\begin{tabular}{c  c  c} \hline
% centering table
% creating 10 columns
% inserting double-line 

Parameters & Class (Letters) & Accuracy \\\hline
\multirow{3}{*}{K = F, D = F} & S & 29.4\% \\
	& H & 30.5\% \\
	& Y & 33.1\% \\\hline
\multirow{3}{*}{K = T, D = F} & H & 57.4\% \\
	& S & 64.2\% \\
	& X & 64.3\% \\\hline
\multirow{3}{*}{K = F, D = T} & H & 57.5\% \\
	& E & 60.7\% \\
	& X & 64.4\% \\\hline
\end{tabular}
\caption{Worst Cases: Naive Bayes} % title name of the table
\label{tbl:bayes}
\end{minipage}
%\end{subtable} 
\end{table}


\subsubsection{Naive Bayes}
Naive Bayes does not work well on the Letter Recognition dataset. From Figure \ref{fig:letter-bayes}, we can see that the highest accuracy is lower than 75\%. Generally speaking, Naive Bayes is not suitable for classifying letters.

In this dataset, we are also interested in which letters Naive Bayes performs worst. Table \ref{tbl:bayes} shows the top 3 worst cases when using Naive Bayes with different parameters. In all three parameter settings, `H' is always one of the top 3, which means that `H' is quite hard to classify for Naive Bayes. Also, the same applies to `S' and `X', as they appear in two of the three cases.


\begin{figure}[htbp]
\centering

\begin{subfigure}[htbp]{0.46\columnwidth}
\includegraphics*[width=\textwidth]{fig/digit_bayes}
\caption{Digit Classification}
\label{fig:digit-bayes}
\end{subfigure}
\hfill
\begin{subfigure}[htbp]{0.46\columnwidth}
\includegraphics*[width=\textwidth]{fig/letter_bayes}
\caption{Letter Recognition}
\label{fig:letter-bayes}
\end{subfigure}
\caption{Performance of Naive Bayes}
\label{fig:bayes}
\end{figure}
\section{Discussion}

\subsection{Evaluation Matrix}
In our presentation, we used \texttt{root relative squared error} to evaluate the performance of our algorithms and falsely claimed that SVM was not suitable for our datasets. However, it makes little sense to evaluate this feature on  non-binary datasets. So in the report, we changed it to F-measure.

\subsection{Different Kernels of SVM}
In our presentation, we only talked about the polynomial kernel. In the report, we added xxx, xxx kernels to compare the performance. By comparison, we can see that ......

Say something Say something Say something Say something Say something Say something Say something Say something Say something Say something Say something Say somet

\subsection{Parameters of Multilayer Perceptron}
Multilayer Perceptron has a number of parameters. In the project, we would like to look into the following specific parameters: learning rate, training time and validation threshold. Other parameters include momentum, seed, nominalToBinaryFilter, hiddenLayers etc.

Learning rate (-L) stands for the amount the weights are updated. Training time (-N) is the number of epochs to train through. If the validation set is non-zero then it can terminate the network early. Validation threshold (-E) is used to terminate validation testing. The value here dictates how many times in a row the validation set error can get worse before training is terminated.

In our tests, the validation threshold does not affect the performance.
 
\section{Conclusion}



\newpage
\bibliographystyle{plain}
\bibliography{Group10}
\end{document}







